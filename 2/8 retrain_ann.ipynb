{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import scanpy as sc\r\n",
    "import anndata\r\n",
    "import torch\r\n",
    "import numpy as np \r\n",
    "import pandas as pd\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch import nn\r\n",
    "from torch import optim\r\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\r\n",
    "from torch.utils.data import DataLoader,TensorDataset,SubsetRandomSampler, WeightedRandomSampler\r\n",
    "\r\n",
    "sc.settings.verbosity = 3\r\n",
    "sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor='white')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "humdata = np.load('5 x_hum_0831_anno.npz', allow_pickle=True)\r\n",
    "x_hum = torch.from_numpy(humdata['x'].astype(np.float32))\r\n",
    "y_hum = torch.from_numpy(humdata['y'].astype(np.int32))\r\n",
    "y_hum = y_hum.long() - 1\r\n",
    "\r\n",
    "inputSize = x_hum.shape[1]\r\n",
    "outputSize = 14\r\n",
    "hiddenSize = 16\r\n",
    "num_epochs = 30\r\n",
    "batch_size = 8\r\n",
    "lr = 0.001\r\n",
    "\r\n",
    "sample_size = 8\r\n",
    "sample_idx=[]\r\n",
    "for t in np.unique(y_hum):\r\n",
    "    t_idx = np.where(y_hum==t)[0]\r\n",
    "    sample_idx.append(np.random.choice(t_idx, size=sample_size))\r\n",
    "sample_idx = np.vstack(sample_idx)\r\n",
    "sample_idx = sample_idx.reshape(sample_idx.size)\r\n",
    "x_train = x_hum[sample_idx]\r\n",
    "y_train = y_hum[sample_idx]\r\n",
    "\r\n",
    "dataset = TensorDataset(x_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class OutputHook(list):\r\n",
    "    def __call__(self, module, input, output):\r\n",
    "        self.append(output)\r\n",
    "        \r\n",
    "class ANN(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(ANN, self).__init__()\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.layers = nn.Sequential(\r\n",
    "            nn.Linear(inputSize, hiddenSize),\r\n",
    "            nn.Dropout(0.5),\r\n",
    "            nn.Linear(hiddenSize, outputSize),\r\n",
    "            self.relu\r\n",
    "        )\r\n",
    "            \r\n",
    "    def forward(self, x):\r\n",
    "        out = self.layers(x)\r\n",
    "        if not self.training:\r\n",
    "            out = F.softmax(out, dim=1)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "device = \"cuda:0\"\r\n",
    "torch.manual_seed(42)\r\n",
    "\r\n",
    "l1_lambda = 1e-3\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "def train(model, device, dataloader, loss_fn, optimizer):\r\n",
    "    train_loss, train_correct=0.0,0\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    for inputs, labels in dataloader:\r\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        output = model(inputs.float())\r\n",
    "\r\n",
    "        l1_penalty = 0.\r\n",
    "        for output in output_hook:\r\n",
    "            l1_penalty += torch.norm(output, 1)\r\n",
    "        l1_penalty *= l1_lambda\r\n",
    "\r\n",
    "        loss = loss_fn(output,labels) + l1_penalty\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        output_hook.clear()\r\n",
    "        \r\n",
    "        train_loss += loss.item() * inputs.size(0)\r\n",
    "        scores, predictions = torch.max(output.data, 1)\r\n",
    "        train_correct += (predictions == labels).sum().item()\r\n",
    "\r\n",
    "    return train_loss,train_correct\r\n",
    "\r\n",
    "def val(model, device, dataloader, loss_fn):\r\n",
    "    valid_loss, val_correct = 0.0, 0\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    for inputs, labels in dataloader:\r\n",
    "        inputs,labels = inputs.to(device),labels.to(device)\r\n",
    "        output = model(inputs)\r\n",
    "        loss = loss_fn(output,labels)\r\n",
    "        valid_loss += loss.item()*inputs.size(0)\r\n",
    "        scores, predictions = torch.max(output.data,1)\r\n",
    "        val_correct += (predictions == labels).sum().item()\r\n",
    "\r\n",
    "    return valid_loss,val_correct\r\n",
    "\r\n",
    "\r\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)\r\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size)\r\n",
    "\r\n",
    "model = torch.load('ANN_0912_equalSample.pt')\r\n",
    "model.to(device)\r\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr)\r\n",
    "\r\n",
    "output_hook = OutputHook()\r\n",
    "model.relu.register_forward_hook(output_hook)\r\n",
    "\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    train_loss, train_correct = train(model, device, train_loader, loss_fn, optimizer)\r\n",
    "    test_loss, test_correct = val(model, device, test_loader, loss_fn)\r\n",
    "\r\n",
    "    train_loss = train_loss / len(train_loader.sampler)\r\n",
    "    train_acc = train_correct / len(train_loader.sampler) * 100\r\n",
    "    test_loss = test_loss / len(test_loader.sampler)\r\n",
    "    test_acc = test_correct / len(test_loader.sampler) * 100\r\n",
    "\r\n",
    "    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\r\n",
    "        epoch + 1, num_epochs, train_loss, test_loss, train_acc, test_acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:1/30 AVG Training Loss:2.788 AVG Test Loss:2.228 AVG Training Acc 25.00 % AVG Test Acc 70.45 %\n",
      "Epoch:2/30 AVG Training Loss:1.657 AVG Test Loss:2.082 AVG Training Acc 51.14 % AVG Test Acc 77.27 %\n",
      "Epoch:3/30 AVG Training Loss:1.271 AVG Test Loss:2.026 AVG Training Acc 65.91 % AVG Test Acc 82.95 %\n",
      "Epoch:4/30 AVG Training Loss:1.212 AVG Test Loss:1.978 AVG Training Acc 68.18 % AVG Test Acc 82.95 %\n",
      "Epoch:5/30 AVG Training Loss:0.939 AVG Test Loss:1.947 AVG Training Acc 77.27 % AVG Test Acc 84.09 %\n",
      "Epoch:6/30 AVG Training Loss:0.940 AVG Test Loss:1.932 AVG Training Acc 77.27 % AVG Test Acc 85.23 %\n",
      "Epoch:7/30 AVG Training Loss:0.826 AVG Test Loss:1.914 AVG Training Acc 79.55 % AVG Test Acc 87.50 %\n",
      "Epoch:8/30 AVG Training Loss:0.713 AVG Test Loss:1.888 AVG Training Acc 87.50 % AVG Test Acc 92.05 %\n",
      "Epoch:9/30 AVG Training Loss:0.736 AVG Test Loss:1.856 AVG Training Acc 82.95 % AVG Test Acc 93.18 %\n",
      "Epoch:10/30 AVG Training Loss:0.592 AVG Test Loss:1.844 AVG Training Acc 88.64 % AVG Test Acc 94.32 %\n",
      "Epoch:11/30 AVG Training Loss:0.558 AVG Test Loss:1.827 AVG Training Acc 89.77 % AVG Test Acc 96.59 %\n",
      "Epoch:12/30 AVG Training Loss:0.504 AVG Test Loss:1.817 AVG Training Acc 90.91 % AVG Test Acc 96.59 %\n",
      "Epoch:13/30 AVG Training Loss:0.496 AVG Test Loss:1.807 AVG Training Acc 92.05 % AVG Test Acc 97.73 %\n",
      "Epoch:14/30 AVG Training Loss:0.433 AVG Test Loss:1.796 AVG Training Acc 97.73 % AVG Test Acc 98.86 %\n",
      "Epoch:15/30 AVG Training Loss:0.420 AVG Test Loss:1.786 AVG Training Acc 93.18 % AVG Test Acc 98.86 %\n",
      "Epoch:16/30 AVG Training Loss:0.416 AVG Test Loss:1.783 AVG Training Acc 94.32 % AVG Test Acc 98.86 %\n",
      "Epoch:17/30 AVG Training Loss:0.336 AVG Test Loss:1.782 AVG Training Acc 96.59 % AVG Test Acc 98.86 %\n",
      "Epoch:18/30 AVG Training Loss:0.406 AVG Test Loss:1.779 AVG Training Acc 96.59 % AVG Test Acc 98.86 %\n",
      "Epoch:19/30 AVG Training Loss:0.426 AVG Test Loss:1.779 AVG Training Acc 95.45 % AVG Test Acc 98.86 %\n",
      "Epoch:20/30 AVG Training Loss:0.415 AVG Test Loss:1.770 AVG Training Acc 95.45 % AVG Test Acc 98.86 %\n",
      "Epoch:21/30 AVG Training Loss:0.364 AVG Test Loss:1.770 AVG Training Acc 96.59 % AVG Test Acc 98.86 %\n",
      "Epoch:22/30 AVG Training Loss:0.380 AVG Test Loss:1.769 AVG Training Acc 95.45 % AVG Test Acc 98.86 %\n",
      "Epoch:23/30 AVG Training Loss:0.310 AVG Test Loss:1.768 AVG Training Acc 96.59 % AVG Test Acc 98.86 %\n",
      "Epoch:24/30 AVG Training Loss:0.420 AVG Test Loss:1.768 AVG Training Acc 94.32 % AVG Test Acc 98.86 %\n",
      "Epoch:25/30 AVG Training Loss:0.306 AVG Test Loss:1.769 AVG Training Acc 96.59 % AVG Test Acc 98.86 %\n",
      "Epoch:26/30 AVG Training Loss:0.284 AVG Test Loss:1.769 AVG Training Acc 97.73 % AVG Test Acc 98.86 %\n",
      "Epoch:27/30 AVG Training Loss:0.357 AVG Test Loss:1.768 AVG Training Acc 95.45 % AVG Test Acc 98.86 %\n",
      "Epoch:28/30 AVG Training Loss:0.289 AVG Test Loss:1.768 AVG Training Acc 97.73 % AVG Test Acc 98.86 %\n",
      "Epoch:29/30 AVG Training Loss:0.329 AVG Test Loss:1.768 AVG Training Acc 94.32 % AVG Test Acc 98.86 %\n",
      "Epoch:30/30 AVG Training Loss:0.309 AVG Test Loss:1.768 AVG Training Acc 98.86 % AVG Test Acc 98.86 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "torch.save(model,'ANN_0912_retrain.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "bd0568d8647bfe2c5b59c81f47863eb65b413eeef312764b5149d804a4a00697"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}