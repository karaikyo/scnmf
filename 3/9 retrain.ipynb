{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import scanpy as sc\r\n",
    "import anndata\r\n",
    "import torch\r\n",
    "import numpy as np \r\n",
    "import pandas as pd\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch import nn\r\n",
    "from torch import optim\r\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\r\n",
    "from torch.utils.data import DataLoader,TensorDataset,SubsetRandomSampler, WeightedRandomSampler\r\n",
    "\r\n",
    "sc.settings.verbosity = 3\r\n",
    "sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor='white')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "humdata = np.load('5 x_hum_0902_anno_nn.npz', allow_pickle=True)\r\n",
    "humnmf = pd.read_pickle('6 Ws_hum_0902')\r\n",
    "x_hum = torch.from_numpy(humnmf[11]).float() # 1,2,5,11\r\n",
    "y_hum = torch.from_numpy(humdata['y'].astype(np.int32))\r\n",
    "y_hum = y_hum.long() - 1\r\n",
    "\r\n",
    "dataset = TensorDataset(x_hum, y_hum)\r\n",
    "\r\n",
    "inputSize = x_hum.shape[1]\r\n",
    "outputSize = 14\r\n",
    "\r\n",
    "num_epochs = 30\r\n",
    "batch_size = 8\r\n",
    "lr = 0.02\r\n",
    "\r\n",
    "sample_size = 8\r\n",
    "sample_idx=[]\r\n",
    "for t in np.unique(y_hum):\r\n",
    "    t_idx = np.where(y_hum==t)[0]\r\n",
    "    sample_idx.append(np.random.choice(t_idx, size=sample_size))\r\n",
    "sample_idx = np.vstack(sample_idx)\r\n",
    "sample_idx = sample_idx.reshape(sample_idx.size)\r\n",
    "x_train = x_hum[sample_idx]\r\n",
    "y_train = y_hum[sample_idx]\r\n",
    "\r\n",
    "dataset = TensorDataset(x_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "class MLR(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(MLR, self).__init__()\r\n",
    "        self.layers = nn.Sequential(\r\n",
    "            nn.Linear(inputSize, outputSize)\r\n",
    "        )\r\n",
    "            \r\n",
    "    def forward(self, x):\r\n",
    "        out = self.layers(x)\r\n",
    "        if not self.training:\r\n",
    "            #print(\"TESTING MODE\")\r\n",
    "            out = F.softmax(out, dim=1)\r\n",
    "        #out = out.view(out.shape[0], -1)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "device = \"cuda:0\"\r\n",
    "torch.manual_seed(42)\r\n",
    "\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "def train(model, device, dataloader, loss_fn, optimizer):\r\n",
    "    train_loss, train_correct=0.0,0\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    for inputs, labels in dataloader:\r\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        output = model(inputs.float())\r\n",
    "\r\n",
    "        loss = loss_fn(output,labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        train_loss += loss.item() * inputs.size(0)\r\n",
    "        scores, predictions = torch.max(output.data, 1)\r\n",
    "        train_correct += (predictions == labels).sum().item()\r\n",
    "\r\n",
    "    return train_loss,train_correct\r\n",
    "\r\n",
    "def val(model, device, dataloader, loss_fn):\r\n",
    "    valid_loss, val_correct = 0.0, 0\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    for inputs, labels in dataloader:\r\n",
    "        inputs,labels = inputs.to(device),labels.to(device)\r\n",
    "        output = model(inputs)\r\n",
    "        loss = loss_fn(output,labels)\r\n",
    "        valid_loss += loss.item()*inputs.size(0)\r\n",
    "        scores, predictions = torch.max(output.data,1)\r\n",
    "        val_correct += (predictions == labels).sum().item()\r\n",
    "\r\n",
    "    return valid_loss,val_correct\r\n",
    "\r\n",
    "\r\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)\r\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size)\r\n",
    "\r\n",
    "model = torch.load('MLR_NMF_r60.pt')\r\n",
    "model.to(device)\r\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr)\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    train_loss, train_correct = train(model, device, train_loader, loss_fn, optimizer)\r\n",
    "    test_loss, test_correct = val(model, device, test_loader, loss_fn)\r\n",
    "\r\n",
    "    train_loss = train_loss / len(train_loader.sampler)\r\n",
    "    train_acc = train_correct / len(train_loader.sampler) * 100\r\n",
    "    test_loss = test_loss / len(test_loader.sampler)\r\n",
    "    test_acc = test_correct / len(test_loader.sampler) * 100\r\n",
    "\r\n",
    "    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\r\n",
    "        epoch + 1, num_epochs, train_loss, test_loss, train_acc, test_acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Miniconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Miniconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:1/30 AVG Training Loss:3.106 AVG Test Loss:2.573 AVG Training Acc 0.00 % AVG Test Acc 19.32 %\n",
      "Epoch:2/30 AVG Training Loss:2.363 AVG Test Loss:2.532 AVG Training Acc 14.77 % AVG Test Acc 32.95 %\n",
      "Epoch:3/30 AVG Training Loss:2.031 AVG Test Loss:2.496 AVG Training Acc 25.00 % AVG Test Acc 45.45 %\n",
      "Epoch:4/30 AVG Training Loss:1.805 AVG Test Loss:2.466 AVG Training Acc 40.91 % AVG Test Acc 55.68 %\n",
      "Epoch:5/30 AVG Training Loss:1.635 AVG Test Loss:2.438 AVG Training Acc 51.14 % AVG Test Acc 64.77 %\n",
      "Epoch:6/30 AVG Training Loss:1.500 AVG Test Loss:2.411 AVG Training Acc 62.50 % AVG Test Acc 80.68 %\n",
      "Epoch:7/30 AVG Training Loss:1.391 AVG Test Loss:2.387 AVG Training Acc 76.14 % AVG Test Acc 86.36 %\n",
      "Epoch:8/30 AVG Training Loss:1.299 AVG Test Loss:2.365 AVG Training Acc 80.68 % AVG Test Acc 90.91 %\n",
      "Epoch:9/30 AVG Training Loss:1.220 AVG Test Loss:2.344 AVG Training Acc 88.64 % AVG Test Acc 96.59 %\n",
      "Epoch:10/30 AVG Training Loss:1.150 AVG Test Loss:2.324 AVG Training Acc 92.05 % AVG Test Acc 98.86 %\n",
      "Epoch:11/30 AVG Training Loss:1.087 AVG Test Loss:2.305 AVG Training Acc 96.59 % AVG Test Acc 98.86 %\n",
      "Epoch:12/30 AVG Training Loss:1.029 AVG Test Loss:2.287 AVG Training Acc 97.73 % AVG Test Acc 98.86 %\n",
      "Epoch:13/30 AVG Training Loss:0.978 AVG Test Loss:2.269 AVG Training Acc 98.86 % AVG Test Acc 100.00 %\n",
      "Epoch:14/30 AVG Training Loss:0.930 AVG Test Loss:2.252 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:15/30 AVG Training Loss:0.886 AVG Test Loss:2.236 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:16/30 AVG Training Loss:0.846 AVG Test Loss:2.221 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:17/30 AVG Training Loss:0.809 AVG Test Loss:2.206 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:18/30 AVG Training Loss:0.774 AVG Test Loss:2.192 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:19/30 AVG Training Loss:0.741 AVG Test Loss:2.178 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:20/30 AVG Training Loss:0.711 AVG Test Loss:2.165 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:21/30 AVG Training Loss:0.683 AVG Test Loss:2.153 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:22/30 AVG Training Loss:0.656 AVG Test Loss:2.141 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:23/30 AVG Training Loss:0.631 AVG Test Loss:2.129 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:24/30 AVG Training Loss:0.607 AVG Test Loss:2.118 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:25/30 AVG Training Loss:0.584 AVG Test Loss:2.107 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:26/30 AVG Training Loss:0.563 AVG Test Loss:2.097 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:27/30 AVG Training Loss:0.543 AVG Test Loss:2.087 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:28/30 AVG Training Loss:0.524 AVG Test Loss:2.077 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:29/30 AVG Training Loss:0.505 AVG Test Loss:2.068 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:30/30 AVG Training Loss:0.488 AVG Test Loss:2.059 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "torch.save(model, 'MLR_NMF_r60_retrain.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "bd0568d8647bfe2c5b59c81f47863eb65b413eeef312764b5149d804a4a00697"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}